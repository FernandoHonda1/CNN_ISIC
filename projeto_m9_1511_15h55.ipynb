{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Conv2D\n",
    "from tensorflow.keras.layers import Flatten, MaxPool2D, AvgPool2D, BatchNormalization \n",
    "from tensorflow.keras.regularizers import l2 \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÕES\n",
    "\n",
    "def get_images_n_labels(dataframe, i, j, path = '17n18_train/', series_name = 'img', label_name = 'pos'):\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for n in range(dataframe.shape[0]):\n",
    "        img = cv2.imread(path + dataframe[series_name].iloc[n])\n",
    "        img = cv2.resize(img, (i, j))\n",
    "        \n",
    "        x.append(img)\n",
    "        y.append(dataframe[label_name].iloc[n])\n",
    "        \n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "# https://stackoverflow.com/questions/25008458/how-to-apply-clahe-on-rgb-color-images\n",
    "def bgr_CLAHE(img):\n",
    "    \n",
    "\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    lab_planes = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit = 2.0,tileGridSize = (6, 6))\n",
    "    lab_planes[0] = clahe.apply(lab_planes[0])\n",
    "    lab = cv2.merge(lab_planes)\n",
    "    img = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# daqui para baixo, funções referentes ao processo de data augmentation\n",
    "# 'zoom' recorte seguido de resize\n",
    "def zoom(img, original_dim = [200, 200], h_slice = [10, 190], v_slice = [10, 190]):\n",
    "\n",
    "    img = img[v_slice[0] : v_slice[1], h_slice[0] : h_slice[1]]\n",
    "    img = cv2.resize(img, (original_dim[0], original_dim[1]))\n",
    "    \n",
    "    return img \n",
    "\n",
    "# horizontal shift\n",
    "def h_shift(image, original_dim = [200, 200], shift = 10):\n",
    "    \n",
    "    T_x = shift\n",
    "    T_y = 0\n",
    "    \n",
    "    M = np.array([[1, 0, T_x], [0, 1, T_y]], dtype = 'float32')\n",
    "    img_transladada = cv2.warpAffine(image, M, (200, 200))\n",
    "    img = img_transladada[0 : original_dim[0], shift : original_dim[1]]\n",
    "    img = cv2.resize(img, (original_dim[0], original_dim[1]))\n",
    "    \n",
    "    return img\n",
    "\n",
    "# vertical shift\n",
    "def v_shift(image, original_dim = [200, 200], shift = 10):\n",
    "    \n",
    "    T_x = 0\n",
    "    T_y = shift\n",
    "    \n",
    "    M = np.array([[1, 0, T_x], [0, 1, T_y]], dtype = 'float32')\n",
    "    img_transladada = cv2.warpAffine(image, M, (200, 200))\n",
    "    img = img_transladada[shift : original_dim[0], 0 : original_dim[1]]\n",
    "    img = cv2.resize(img, (original_dim[0], original_dim[1]))\n",
    "    \n",
    "    return img\n",
    "\n",
    "def rotation_90(img):\n",
    "\n",
    "    rows, cols, chnls = img.shape\n",
    "    M = cv2.getRotationMatrix2D(((cols - 1) / 2.0, (rows - 1) / 2.0), 90, 1)\n",
    "    img = cv2.warpAffine(img, M, (cols, rows))\n",
    "    \n",
    "    return img\n",
    "\n",
    "def data_augmentation(x, y):\n",
    "    \n",
    "    augmentation_imgs = []\n",
    "    augmentation_labels = []\n",
    "\n",
    "    for n in range(len(x)):\n",
    "\n",
    "        image = x[n]\n",
    "        classe = y[n]\n",
    "        augment = randint(0, 1) # booleano (50% de chance de aplicar augmentation)\n",
    "\n",
    "        if augment == 1:\n",
    "            process = randint(0, 5) # seleção aleatória do processo de augmentation\n",
    "\n",
    "            if process == 0:\n",
    "                image = cv2.flip(image, 0)\n",
    "                augmentation_imgs.append(image) # horizontal flip\n",
    "                augmentation_labels.append(classe)\n",
    "\n",
    "            if process == 1:\n",
    "                image = zoom(image)\n",
    "                augmentation_imgs.append(image) # zoom 0.2\n",
    "                augmentation_labels.append(classe)\n",
    "\n",
    "            if process == 2:\n",
    "                image = h_shift(image)\n",
    "                augmentation_imgs.append(image) # horizontal shift\n",
    "                augmentation_labels.append(classe)\n",
    "\n",
    "            if process == 3:\n",
    "                image = v_shift(image)\n",
    "                augmentation_imgs.append(image) # vertical shift\n",
    "                augmentation_labels.append(classe)\n",
    "\n",
    "            if process == 4:\n",
    "                image = rotation_90(image)\n",
    "                augmentation_imgs.append(image) # rotaton 90°\n",
    "                augmentation_labels.append(classe)\n",
    "                \n",
    "    return augmentation_imgs, augmentation_labels\n",
    "\n",
    "# este processo deve rodar uma vez com cada um dos batches acima, logo em seguida, o resultado da operação\n",
    "# deve passar pela rede\n",
    "def single_batch_prep(batch_df, width, height):\n",
    "    # normalização ?\n",
    "    x, y = get_images_n_labels(batch_df, width, height)\n",
    "\n",
    "    # pré processamento de imagens\n",
    "    x = [bgr_CLAHE(i) for i in x]\n",
    "    x = [cv2.fastNlMeansDenoisingColored(i) for i in x]\n",
    "#     x = [cv2.cvtColor(i, cv2.COLOR_BGR2GRAY) for i in x] # precisa fazer isso ?\n",
    "\n",
    "    # data augmentation\n",
    "    augmentation_imgs, augmentation_labels = data_augmentation(x, y)\n",
    "    \n",
    "    return np.array(x + augmentation_imgs), np.array(list(y) + augmentation_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDES (autoencoder e nn)\n",
    "\n",
    "# auto encoder (pré processamento)\n",
    "ae_i, ae_j, chnls = 200, 200, 3\n",
    "ipt_e = Input(shape = (px_h, px_v, n_canais))\n",
    "\n",
    "encoder = Conv2D(4, (3, 3), input_shape = (px_h, px_v, n_canais), activation = 'relu', padding = 'same')(ipt_e)\n",
    "encoder = MaxPooling2D((2, 2))(encoder)\n",
    "encoder = Conv2D(8, (3, 3), activation = 'relu', padding = 'same')(encoder)\n",
    "encoder = MaxPooling2D((2, 2))(encoder)\n",
    "encoder = Conv2D(16, (2, 2), activation = 'relu', padding = 'same')(encoder)\n",
    "encoder = Flatten()(encoder)\n",
    "encoder = Dense(32, name = 'gargalo')(encoder)\n",
    "\n",
    "decoder = Dense(784)(encoder)\n",
    "decoder = Reshape(target_shape = (7, 7, 16))(decoder)\n",
    "decoder = Conv2D(16, (2, 2), activation = 'relu', padding = 'same')(decoder)\n",
    "decoder = UpSampling2D((2, 2))(decoder)\n",
    "decoder = Conv2D(8, (3, 3), activation = 'relu', padding = 'same')(decoder)\n",
    "decoder = UpSampling2D((2, 2))(decoder)\n",
    "decoder = Conv2D(4, (3, 3), activation = 'relu', padding = 'same')(decoder)\n",
    "decoder = Conv2D(1, (3, 3), activation = 'sigmoid', padding = 'same')(decoder)\n",
    "\n",
    "autoencoder = Model(inputs = ipt_e, outputs = decoder, name = 'autoencoder')\n",
    "autoencoder.compile(loss = 'mean_squared_error')\n",
    "\n",
    "# autoencoder.save('autoencoder.h5') \n",
    "\n",
    "# alterar arquitetura (não deve ser convolucional)\n",
    "# nn (predição)\n",
    "cnn_i, cnn_j, cnn_chnls = 200, 200, 3\n",
    "\n",
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(50, (3, 3), input_shape = (i, j, chnls), activation = 'relu'))\n",
    "cnn.add(Conv2D(50, (3, 3), input_shape = (i, j, chnls), activation = 'relu'))\n",
    "cnn.add(MaxPool2D((2, 2)))\n",
    "cnn.add(Conv2D(50, (3, 3), input_shape = (i, j, chnls), activation = 'relu'))\n",
    "cnn.add(Conv2D(50, (3, 3), input_shape = (i, j, chnls), activation = 'relu'))\n",
    "cnn.add(AvgPool2D((2, 2)))\n",
    "cnn.add(Conv2D(50, (3, 3), input_shape = (i, j, chnls), activation = 'relu'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(AvgPool2D((2, 2)))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(25, activation = 'relu', kernel_regularizer = l2(0.05)))\n",
    "cnn.add(Dropout(0.2))\n",
    "cnn.add(Dense(25, activation = 'relu', kernel_regularizer = l2(0.05)))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Dropout(0.2))\n",
    "cnn.add(Dense(25, activation = 'relu', kernel_regularizer = l2(0.05)))\n",
    "cnn.add(Dropout(0.2))\n",
    "cnn.add(Dense(25, activation = 'relu', kernel_regularizer = l2(0.05)))\n",
    "cnn.add(Dense(1, activation = 'sigmoid'))\n",
    "        \n",
    "cnn.compile(loss = 'binary_crossentropy', optimizer = RMSprop(learning_rate = 0.001))\n",
    "\n",
    "es = EarlyStopping(monitor = 'loss', patience = 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARREGANDO E TRATANDO DESCRITIVOS (treino e validação)\n",
    "\n",
    "df_validacao_2017 = pd.read_csv('isic_2017_validacao.csv')\n",
    "\n",
    "# VALIDAÇÃO (extração e pré processamento)\n",
    "\n",
    "df_validacao_2017.drop('seborrheic_keratosis', axis = 1, inplace = True)\n",
    "df_validacao_2017.columns = ['img', 'pos']\n",
    "df_validacao_2017['img'] = df_validacao_2017['img'].apply(lambda x: x + '.jpg')\n",
    "\n",
    "# tirando slice, para ser mais rápido\n",
    "df_validacao_2017.sample(50, random_state = 123)\n",
    "\n",
    "x_val, y_val = get_images_n_labels(df_validacao_2017, 200, 200, path = 'isic_2017_validacao/')\n",
    "\n",
    "# pré processamento de imagens\n",
    "x_val = [bgr_CLAHE(i) for i in x_val]\n",
    "x_val = [cv2.fastNlMeansDenoisingColored(i) for i in x_val]\n",
    "\n",
    "df_treino_2017 = pd.read_csv('isic_2017_treino.csv')\n",
    "df_treino_2018 = pd.read_csv('isic_2018_treino.csv')\n",
    "\n",
    "# TRATAMENTO DO DESCRITIVO (treino '17 '18)\n",
    "\n",
    "# tratando 2017\n",
    "\n",
    "# adicionando extensão aos nomes dos arquivos\n",
    "df_treino_2017['image_id'] = df_treino_2017['image_id'].apply(lambda x: x + '.jpg')\n",
    "\n",
    "# criando flag para nevus\n",
    "df_treino_2017['nevus'] = ((df_treino_2017.melanoma == 0) & \n",
    "                           (df_treino_2017.seborrheic_keratosis == 0)).astype(float)\n",
    "\n",
    "df_treino_2017.drop('seborrheic_keratosis', axis = 1, inplace = True)\n",
    "\n",
    "# as imagens precisam ser ou nevus ou melanoma\n",
    "df_treino_2017 = df_treino_2017.loc[(df_treino_2017['melanoma'] == 1) | (df_treino_2017['nevus'] == 1)]\n",
    "\n",
    "# minha pasta 'isic_2017_treino' possui 450 imagens (algumas destas, não são nevus ou melanoma)\n",
    "\n",
    "dir_treino_17 = r'C:\\Users\\fkhon\\Documents\\LETSCODE\\MODULO9\\projeto_m9_1311\\isic_2017_treino'\n",
    "dir_treino_17_filenames = [name for name in os.listdir(dir_treino_17)]\n",
    "\n",
    "# eliminarei do descritivo, os nomes que não constam na pasta 'isic_2017_treino'\n",
    "df_treino_2017 = \\\n",
    "  df_treino_2017.loc[df_treino_2017['image_id'].isin(dir_treino_17_filenames)]\n",
    "\n",
    "# tratando 2018\n",
    "# tratando nomes (adicionando extensões aos nomes) e filtrando series \n",
    "df_treino_2018 = df_treino_2018[['image', 'MEL', 'NV']]\n",
    "df_treino_2018['image'] = df_treino_2018['image'].apply(lambda x: x + '.jpg')\n",
    "\n",
    "# verificando correspondência entre descritivo e pasta\n",
    "dir_treino_18 = r'C:\\Users\\fkhon\\Documents\\LETSCODE\\MODULO9\\projeto_m9_1311\\isic_2018_treino'\n",
    "dir_treino_18_filenames = [name for name in os.listdir(dir_treino_18)]\n",
    "\n",
    "# imagens precisam ser ou nevus ou melanoma\n",
    "df_treino_2018 = df_treino_2018.loc[(df_treino_2018['MEL'] == 1) | (df_treino_2018['NV'] == 1)]\n",
    "\n",
    "# unificar nomes das series, para eventual concat\n",
    "df_treino_2018.columns = ['img', 'pos', 'neg']\n",
    "df_treino_2017.columns = ['img', 'pos', 'neg']\n",
    "\n",
    "# juntar as duas bases, a fim de somar 1200 imagens de cada caso\n",
    "# para tanto, somarei os casos positivos de 2017 aos de 2018, e eliminarei de 2018, casos negativos\n",
    "# suficientes para que o total se equipare ao de positivos\n",
    "\n",
    "# separando casos positivos e negativos de cada ano\n",
    "pos_2018 = df_treino_2018.loc[df_treino_2018['pos'] == 1]\n",
    "pos_2017 = df_treino_2017.loc[df_treino_2017['pos'] == 1]\n",
    "\n",
    "neg_2018 = df_treino_2018.loc[df_treino_2018['neg'] == 1]\n",
    "neg_2017 = df_treino_2017.loc[df_treino_2017['neg'] == 1]\n",
    "\n",
    "# juntando casos positivos e negativos de todos os anos\n",
    "full_pos = pd.concat([pos_2017, pos_2018], axis = 0)\n",
    "full_neg = pd.concat([neg_2017, neg_2018], axis = 0)\n",
    "\n",
    "# amostrando aleatoriamente, 1204 dados negativos, estes consistirão nas observações negativas (a amostragem\n",
    "# é feita para separar um número de imagens equivalente ao número de imagens positivas que temos à disposição)\n",
    "full_neg = full_neg.sample(n = full_pos.shape[0], random_state = 123)\n",
    "\n",
    "full_pos = full_pos[['img', 'pos']]\n",
    "full_neg = full_neg[['img', 'pos']]\n",
    "\n",
    "filenames = pd.concat([full_pos, full_neg]).reset_index().drop('index', axis = 1)\n",
    "\n",
    "# daqui para baixo, 'sample' representará o dataframe contendo os nomes de todas as imagens que serão \n",
    "# passadas pela rede\n",
    "\n",
    "sample = filenames.sample(150)\n",
    "sample = sample.reset_index()\n",
    "sample = sample.drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AQUI: fracionar dados, para que possam ser passados pelo processo abaixo em 'batches'\n",
    "\n",
    "batches = 5\n",
    "\n",
    "step = sample.shape[0] // batches\n",
    "count = 0\n",
    "lower_idx = 0\n",
    "\n",
    "for n in range(batches):\n",
    "    \n",
    "    upper_idx = lower_idx + step \n",
    "    \n",
    "    exec(f'batch_{count} = sample.iloc[{lower_idx}:{upper_idx}]')\n",
    "    \n",
    "    count += 1\n",
    "    lower_idx += step\n",
    "    \n",
    "# aqui, realizamos o fracionamento do dataframe contendo os nomes e labels das imagens\n",
    "# exemplo:\n",
    "# batch_0 = dataframe.iloc[0:30]\n",
    "# batch_1 = dataframe.iloc[30:60]\n",
    "# batch_2 = dataframe.iloc[60:90]\n",
    "# batch_3 = dataframe.iloc[90:120]\n",
    "# batch_4 = dataframe.iloc[120:150]\n",
    "\n",
    "# as próximas células fazem o ajuste do codificador a cada batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(batch_0, open('batch_0.pkl', 'wb'))\n",
    "pickle.dump(batch_1, open('batch_1.pkl', 'wb'))\n",
    "pickle.dump(batch_2, open('batch_2.pkl', 'wb'))\n",
    "pickle.dump(batch_3, open('batch_3.pkl', 'wb'))\n",
    "pickle.dump(batch_4, open('batch_4.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nas próximas células, lembrar que seria como se cada célula rodasse de forma isolada, as iformações\n",
    "# necessárias em cada célula precisarão ser carregadas de pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = load_model('autoencoder.h5')\n",
    "# batch_0\n",
    "batch_0 = pickle.load(open('batch_0.pkl', 'rb'))\n",
    "\n",
    "# carregando primeiro batch\n",
    "x_0, y_0 = single_batch_prep(batch_0, 200, 200)\n",
    "\n",
    "# ajustando encoder ao primeiro batch\n",
    "autoencoder.fit(x_0, x_0, batch_size = 10, epochs = 50, validation_data = (np.array(x_val), x_val))\n",
    "\n",
    "autoencoder.save('autoencoder.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_1 \n",
    "\n",
    "# carregando segundo batch\n",
    "x_1, y_1 = single_batch_prep(batch_1, 200, 200)\n",
    "\n",
    "# ajustando encoder ao segundo batch\n",
    "autoencoder.fit(x_1, x_1, batch_size = 10, epochs = 50, validation_data = (np.array(x_val), x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_2\n",
    "\n",
    "# carregando terceiro batch\n",
    "x_2, y_2 = single_batch_prep(batch_2, 200, 200)\n",
    "\n",
    "# ajustando encoder ao terceiro batch\n",
    "autoencoder.fit(x_2, x_2, batch_size = 10, epochs = 50, validation_data = (np.array(x_val), x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_3\n",
    "\n",
    "# carregando quarto batch\n",
    "x_3, y_3 = single_batch_prep(batch_3, 200, 200)\n",
    "\n",
    "# ajustando encoder ao quarto batch\n",
    "autoencoder.fit(x_3, x_3, batch_size = 10, epochs = 50, validation_data = (np.array(x_val), x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_4\n",
    "\n",
    "# carregando quinto batch\n",
    "x_4, y_4 = single_batch_prep(batch_4, 200, 200)\n",
    "\n",
    "# ajustando encoder ao quinto batch\n",
    "autoencoder.fit(x_4, x_4, batch_size = 10, epochs = 50, validation_data = (np.array(x_val), x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declarando encoder (fará redução de dimensionalidade)\n",
    "encoder = Model(inputs = autoencoder.input, outputs = autoencoder.get_layer('gargalo').output)\n",
    "\n",
    "# pré processando dados de validação\n",
    "x_val_cod = encoder.predict(np.array(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daqui para baixo, ajustando rede preditiva a cada batch (cada batch é pré processado com encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando primeiro batch\n",
    "x_0, y_0 = single_batch_prep(batch_0, 200, 200)\n",
    "\n",
    "# treinando predição no primeiro batch\n",
    "x_0_cod = encoder.predict(x_0)\n",
    "cnn.fit(x = x_0_cod, y = y_0, epochs = 50, callbacks = [es], batch_size = 3 , \n",
    "        validation_data = (np.array(x_val_cod), y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando segundo batch\n",
    "x_1, y_1 = single_batch_prep(batch_1, 200, 200)\n",
    "\n",
    "# treinando predição no segundo batch\n",
    "x_1_cod = encoder.predict(x_1)\n",
    "cnn.fit(x = x_1_cod, y = y_1, epochs = 50, callbacks = [es], batch_size = 3, \n",
    "        validation_data = (np.array(x_val_cod), y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando terceiro batch\n",
    "x_2, y_2 = single_batch_prep(batch_2, 200, 200)\n",
    "\n",
    "# treinando predição no terceiro batch\n",
    "x_2_cod = encoder.predict(x_2)\n",
    "cnn.fit(x = x_2_cod, y = y_2, epochs = 50, callbacks = [es], batch_size = 3, \n",
    "        validation_data = (np.array(x_val_cod), y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando quarto batch\n",
    "x_3, y_3 = single_batch_prep(batch_3, 200, 200)\n",
    "\n",
    "# treinando predição no quarto batch\n",
    "x_3_cod = encoder.predict(x_3)\n",
    "cnn.fit(x = x_3_cod, y = y_3, epochs = 50, callbacks = [es], batch_size = 3, \n",
    "        validation_data = (np.array(x_val_cod), y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando quinto batch\n",
    "x_4, y_4 = single_batch_prep(batch_4, 200, 200)\n",
    "\n",
    "# treinando predição no quinto batch\n",
    "x_4_cod = encoder.predict(x_4)\n",
    "cnn.fit(x = x_4_cod, y = y_4, epochs = 50, callbacks = [es], batch_size = 3, \n",
    "        validation_data = (np.array(x_val_cod), y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
