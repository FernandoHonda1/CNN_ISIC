{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Conv2D\n",
    "from tensorflow.keras.layers import Flatten, MaxPool2D, AvgPool2D, BatchNormalization \n",
    "from tensorflow.keras.regularizers import l2 \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "df_treino_2017 = pd.read_csv('isic_2017_treino.csv')\n",
    "df_treino_2018 = pd.read_csv('isic_2018_treino.csv')\n",
    "\n",
    "# ctrl c ctrl v\n",
    "i, j, chnls = 200, 200, 3\n",
    "\n",
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(50, (3, 3), input_shape = (i, j, chnls), activation = 'relu'))\n",
    "cnn.add(Conv2D(50, (3, 3), input_shape = (i, j, chnls), activation = 'relu'))\n",
    "cnn.add(MaxPool2D((2, 2)))\n",
    "cnn.add(Conv2D(50, (3, 3), input_shape = (i, j, chnls), activation = 'relu'))\n",
    "cnn.add(Conv2D(50, (3, 3), input_shape = (i, j, chnls), activation = 'relu'))\n",
    "cnn.add(AvgPool2D((2, 2)))\n",
    "cnn.add(Conv2D(50, (3, 3), input_shape = (i, j, chnls), activation = 'relu'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(AvgPool2D((2, 2)))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(25, activation = 'relu', kernel_regularizer = l2(0.05)))\n",
    "cnn.add(Dropout(0.2))\n",
    "cnn.add(Dense(25, activation = 'relu', kernel_regularizer = l2(0.05)))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Dropout(0.2))\n",
    "cnn.add(Dense(25, activation = 'relu', kernel_regularizer = l2(0.05)))\n",
    "cnn.add(Dropout(0.2))\n",
    "cnn.add(Dense(25, activation = 'relu', kernel_regularizer = l2(0.05)))\n",
    "cnn.add(Dense(1, activation = 'sigmoid'))\n",
    "        \n",
    "cnn.compile(loss = 'binary_crossentropy', optimizer = RMSprop(learning_rate = 0.001))\n",
    "\n",
    "es = EarlyStopping(monitor = 'loss', patience = 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÕES\n",
    "\n",
    "def get_images_n_labels(dataframe, i, j, path = '17n18_train/', series_name = 'img', label_name = 'pos'):\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for n in range(dataframe.shape[0]):\n",
    "        img = cv2.imread(path + dataframe[series_name].iloc[n])\n",
    "        img = cv2.resize(img, (i, j))\n",
    "        \n",
    "        x.append(img)\n",
    "        y.append(dataframe[label_name].iloc[n])\n",
    "        \n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "# https://stackoverflow.com/questions/25008458/how-to-apply-clahe-on-rgb-color-images\n",
    "def bgr_CLAHE(img):\n",
    "    \n",
    "\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    lab_planes = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit = 2.0,tileGridSize = (6, 6))\n",
    "    lab_planes[0] = clahe.apply(lab_planes[0])\n",
    "    lab = cv2.merge(lab_planes)\n",
    "    img = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# daqui para baixo, funções referentes ao processo de data augmentation\n",
    "# 'zoom' recorte seguido de resize\n",
    "def zoom(img, original_dim = [200, 200], h_slice = [10, 190], v_slice = [10, 190]):\n",
    "\n",
    "    img = img[v_slice[0] : v_slice[1], h_slice[0] : h_slice[1]]\n",
    "    img = cv2.resize(img, (original_dim[0], original_dim[1]))\n",
    "    \n",
    "    return img \n",
    "\n",
    "# horizontal shift\n",
    "def h_shift(image, original_dim = [200, 200], shift = 10):\n",
    "    \n",
    "    T_x = shift\n",
    "    T_y = 0\n",
    "    \n",
    "    M = np.array([[1, 0, T_x], [0, 1, T_y]], dtype = 'float32')\n",
    "    img_transladada = cv2.warpAffine(image, M, (200, 200))\n",
    "    img = img_transladada[0 : original_dim[0], shift : original_dim[1]]\n",
    "    img = cv2.resize(img, (original_dim[0], original_dim[1]))\n",
    "    \n",
    "    return img\n",
    "\n",
    "# vertical shift\n",
    "def v_shift(image, original_dim = [200, 200], shift = 10):\n",
    "    \n",
    "    T_x = 0\n",
    "    T_y = shift\n",
    "    \n",
    "    M = np.array([[1, 0, T_x], [0, 1, T_y]], dtype = 'float32')\n",
    "    img_transladada = cv2.warpAffine(image, M, (200, 200))\n",
    "    img = img_transladada[shift : original_dim[0], 0 : original_dim[1]]\n",
    "    img = cv2.resize(img, (original_dim[0], original_dim[1]))\n",
    "    \n",
    "    return img\n",
    "\n",
    "def rotation_90(img):\n",
    "\n",
    "    rows, cols, chnls = img.shape\n",
    "    M = cv2.getRotationMatrix2D(((cols - 1) / 2.0, (rows - 1) / 2.0), 90, 1)\n",
    "    img = cv2.warpAffine(img, M, (cols, rows))\n",
    "    \n",
    "    return img\n",
    "\n",
    "def data_augmentation(x, y):\n",
    "    \n",
    "    augmentation_imgs = []\n",
    "    augmentation_labels = []\n",
    "\n",
    "    for n in range(len(x)):\n",
    "\n",
    "        image = x[n]\n",
    "        classe = y[n]\n",
    "        augment = randint(0, 1) # booleano (50% de chance de aplicar augmentation)\n",
    "\n",
    "        if augment == 1:\n",
    "            process = randint(0, 5) # seleção aleatória do processo de augmentation\n",
    "\n",
    "            if process == 0:\n",
    "                image = cv2.flip(image, 0)\n",
    "                augmentation_imgs.append(image) # horizontal flip\n",
    "                augmentation_labels.append(classe)\n",
    "\n",
    "            if process == 1:\n",
    "                image = zoom(image)\n",
    "                augmentation_imgs.append(image) # zoom 0.2\n",
    "                augmentation_labels.append(classe)\n",
    "\n",
    "            if process == 2:\n",
    "                image = h_shift(image)\n",
    "                augmentation_imgs.append(image) # horizontal shift\n",
    "                augmentation_labels.append(classe)\n",
    "\n",
    "            if process == 3:\n",
    "                image = v_shift(image)\n",
    "                augmentation_imgs.append(image) # vertical shift\n",
    "                augmentation_labels.append(classe)\n",
    "\n",
    "            if process == 4:\n",
    "                image = rotation_90(image)\n",
    "                augmentation_imgs.append(image) # rotaton 90°\n",
    "                augmentation_labels.append(classe)\n",
    "                \n",
    "    return augmentation_imgs, augmentation_labels\n",
    "\n",
    "# este processo deve rodar uma vez com cada um dos batches acima, logo em seguida, o resultado da operação\n",
    "# deve passar pela rede\n",
    "def single_batch_prep(batch_df, width, height):\n",
    "    # normalização ?\n",
    "    x, y = get_images_n_labels(batch_df, width, height)\n",
    "\n",
    "    # pré processamento de imagens\n",
    "    x = [bgr_CLAHE(i) for i in x]\n",
    "    x = [cv2.fastNlMeansDenoisingColored(i) for i in x]\n",
    "#     x = [cv2.cvtColor(i, cv2.COLOR_BGR2GRAY) for i in x] # precisa fazer isso ?\n",
    "\n",
    "    # data augmentation\n",
    "    augmentation_imgs, augmentation_labels = data_augmentation(x, y)\n",
    "    \n",
    "    return np.array(x + augmentation_imgs), np.array(list(y) + augmentation_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRATAMENTO\n",
    "\n",
    "# tratando 2017\n",
    "\n",
    "# adicionando extensão aos nomes dos arquivos\n",
    "df_treino_2017['image_id'] = df_treino_2017['image_id'].apply(lambda x: x + '.jpg')\n",
    "\n",
    "# criando flag para nevus\n",
    "df_treino_2017['nevus'] = ((df_treino_2017.melanoma == 0) & \n",
    "                           (df_treino_2017.seborrheic_keratosis == 0)).astype(float)\n",
    "\n",
    "df_treino_2017.drop('seborrheic_keratosis', axis = 1, inplace = True)\n",
    "\n",
    "# as imagens precisam ser ou nevus ou melanoma\n",
    "df_treino_2017 = df_treino_2017.loc[(df_treino_2017['melanoma'] == 1) | (df_treino_2017['nevus'] == 1)]\n",
    "\n",
    "# minha pasta 'isic_2017_treino' possui 450 imagens (algumas destas, não são nevus ou melanoma)\n",
    "\n",
    "dir_treino_17 = r'C:\\Users\\fkhon\\Documents\\LETSCODE\\MODULO9\\projeto_m9_1311\\isic_2017_treino'\n",
    "dir_treino_17_filenames = [name for name in os.listdir(dir_treino_17)]\n",
    "\n",
    "# eliminarei do descritivo, os nomes que não constam na pasta 'isic_2017_treino'\n",
    "df_treino_2017 = \\\n",
    "  df_treino_2017.loc[df_treino_2017['image_id'].isin(dir_treino_17_filenames)]\n",
    "\n",
    "# tratando 2018\n",
    "# tratando nomes (adicionando extensões aos nomes) e filtrando series \n",
    "df_treino_2018 = df_treino_2018[['image', 'MEL', 'NV']]\n",
    "df_treino_2018['image'] = df_treino_2018['image'].apply(lambda x: x + '.jpg')\n",
    "\n",
    "# verificando correspondência entre descritivo e pasta\n",
    "dir_treino_18 = r'C:\\Users\\fkhon\\Documents\\LETSCODE\\MODULO9\\projeto_m9_1311\\isic_2018_treino'\n",
    "dir_treino_18_filenames = [name for name in os.listdir(dir_treino_18)]\n",
    "\n",
    "# imagens precisam ser ou nevus ou melanoma\n",
    "df_treino_2018 = df_treino_2018.loc[(df_treino_2018['MEL'] == 1) | (df_treino_2018['NV'] == 1)]\n",
    "\n",
    "# unificar nomes das series, para eventual concat\n",
    "df_treino_2018.columns = ['img', 'pos', 'neg']\n",
    "df_treino_2017.columns = ['img', 'pos', 'neg']\n",
    "\n",
    "# juntar as duas bases, a fim de somar 1200 imagens de cada caso\n",
    "# para tanto, somarei os casos positivos de 2017 aos de 2018, e eliminarei de 2018, casos negativos\n",
    "# suficientes para que o total se equipare ao de positivos\n",
    "\n",
    "# separando casos positivos e negativos de cada ano\n",
    "pos_2018 = df_treino_2018.loc[df_treino_2018['pos'] == 1]\n",
    "pos_2017 = df_treino_2017.loc[df_treino_2017['pos'] == 1]\n",
    "\n",
    "neg_2018 = df_treino_2018.loc[df_treino_2018['neg'] == 1]\n",
    "neg_2017 = df_treino_2017.loc[df_treino_2017['neg'] == 1]\n",
    "\n",
    "# juntando casos positivos e negativos de todos os anos\n",
    "full_pos = pd.concat([pos_2017, pos_2018], axis = 0)\n",
    "full_neg = pd.concat([neg_2017, neg_2018], axis = 0)\n",
    "\n",
    "# amostrando aleatoriamente, 1204 dados negativos, estes consistirão nas observações negativas (a amostragem\n",
    "# é feita para separar um número de imagens equivalente ao número de imagens positivas que temos à disposição)\n",
    "full_neg = full_neg.sample(n = full_pos.shape[0], random_state = 123)\n",
    "\n",
    "full_pos = full_pos[['img', 'pos']]\n",
    "full_neg = full_neg[['img', 'pos']]\n",
    "\n",
    "filenames = pd.concat([full_pos, full_neg]).reset_index().drop('index', axis = 1)\n",
    "\n",
    "sample = filenames.sample(150)\n",
    "sample = sample.reset_index()\n",
    "sample = sample.drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDAÇÃO (extração e pré processamento)\n",
    "\n",
    "df_validacao_2017 = pd.read_csv('isic_2017_validacao.csv')\n",
    "df_validacao_2017.drop('seborrheic_keratosis', axis = 1, inplace = True)\n",
    "df_validacao_2017.columns = ['img', 'pos']\n",
    "df_validacao_2017['img'] = df_validacao_2017['img'].apply(lambda x: x + '.jpg')\n",
    "\n",
    "# tirando slice, para ser mais rápido\n",
    "df_validacao_2017.sample(50, random_state = 123)\n",
    "\n",
    "x_val, y_val = get_images_n_labels(df_validacao_2017, 200, 200, path = 'isic_2017_validacao/')\n",
    "\n",
    "# pré processamento de imagens\n",
    "x_val = [bgr_CLAHE(i) for i in x_val]\n",
    "x_val = [cv2.fastNlMeansDenoisingColored(i) for i in x_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daqui para baixo, 'sample' representará o dataframe contendo os nomes de todas as imagens que serão \n",
    "# passadas pela rede\n",
    "\n",
    "# começando já na próxima célula, devemos fracionar os dados (batches)\n",
    "# a função 'get_images_n_labels()' deverá receber frações do dataframe, estas que serão pré processadas e \n",
    "# passarão pelo processo de augmentation em 'turnos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AQUI: fracionar dados, para que possam ser passados pelo processo abaixo em 'batches'\n",
    "\n",
    "batches = 5\n",
    "step = sample.shape[0] // batches\n",
    "\n",
    "count = 0\n",
    "lower_idx = 0\n",
    "dataframe = sample\n",
    "\n",
    "for n in range(batches):\n",
    "    \n",
    "    upper_idx = lower_idx + step \n",
    "    \n",
    "    exec(f'batch_{count} = dataframe.iloc[{lower_idx}:{upper_idx}]')\n",
    "    \n",
    "    count += 1\n",
    "    lower_idx += step\n",
    "    \n",
    "# aqui, realizamos o fracionamento do dataframe contendo os nomes e labels das imagens\n",
    "# exemplo:\n",
    "# batch_0 = dataframe.iloc[0:30]\n",
    "# batch_1 = dataframe.iloc[30:60]\n",
    "# batch_2 = dataframe.iloc[60:90]\n",
    "# batch_3 = dataframe.iloc[90:120]\n",
    "# batch_4 = dataframe.iloc[120:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retorna batches (com base nos dataframes, deta deve rodar uma vez para cada dataframe)\n",
    "x_0, y_0 = single_batch_prep(batch_0, 200, 200)\n",
    "x_1, y_1 = single_batch_prep(batch_1, 200, 200)\n",
    "# x_2, y_2 = single_batch_prep(batch_2, 200, 200)\n",
    "# x_3, y_3 = single_batch_prep(batch_3, 200, 200)\n",
    "# x_4, y_4 = single_batch_prep(batch_4, 200, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acho que aqui, teríamos a rede, esta receberia uma porção de dados recém extraída, pré processada e \n",
    "# 'aumentada', então, após o ajuste a porção em questão, faríamos a extração, pré processamento e 'aumento'\n",
    "# de outra porção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treinando no batch_0\n",
    "cnn.fit(x = x_0, y = y_0, epochs = 50, callbacks = [es], batch_size = 3 , validation_data = (np.array(x_val),\n",
    "                                                                                             y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treinando no batch_1\n",
    "cnn.fit(x = x_1, y = y_1, epochs = 50, callbacks = [es], batch_size = 3, validation_data = (np.array(x_val), \n",
    "                                                                                            y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
